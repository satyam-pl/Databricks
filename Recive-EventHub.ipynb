{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578b6511-3957-49d7-9a6e-18161229f5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2782756541997542>, line 18\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=1FRea6q6kpc4QP1Cfs83BofK5/TXSPDdh+AEhBFwpEg=;EntityPath=at_loandaytransactions\"\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m \n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=72vrKR8yLJ+hY7WI8QnVqFeLCeP1IKg5g+AEhDZc1sI=;EntityPath=clientloanproposal\"\u001B[39;00m\n",
       "\u001B[1;32m     17\u001B[0m ehConf \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[0;32m---> 18\u001B[0m ehConf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventhubs.connectionString\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39meventhubs\u001B[38;5;241m.\u001B[39mEventHubsUtils\u001B[38;5;241m.\u001B[39mencrypt(connectionString)\n",
       "\u001B[1;32m     19\u001B[0m ehConf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventhubs.consumerGroup\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m$Default\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Read streaming data from Event Hubs\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2782756541997542>, line 18\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=1FRea6q6kpc4QP1Cfs83BofK5/TXSPDdh+AEhBFwpEg=;EntityPath=at_loandaytransactions\"\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=72vrKR8yLJ+hY7WI8QnVqFeLCeP1IKg5g+AEhDZc1sI=;EntityPath=clientloanproposal\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m ehConf \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m---> 18\u001B[0m ehConf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventhubs.connectionString\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39meventhubs\u001B[38;5;241m.\u001B[39mEventHubsUtils\u001B[38;5;241m.\u001B[39mencrypt(connectionString)\n\u001B[1;32m     19\u001B[0m ehConf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventhubs.consumerGroup\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m$Default\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Read streaming data from Event Hubs\u001B[39;00m\n\n\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'JavaPackage' object is not callable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# insert in delta table in normal way\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from pyspark.sql.functions import get_json_object\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)\n",
    "\n",
    "connectionString = \"Endpoint=sb://eventhub-cloud.servicebus.windows.net/;SharedAccessKeyName=newStudent;SharedAccessKey=wT5I/O5XMbt7Zpb9SO7qNtlyazfTleLPQ+AEhCrGg7c=;EntityPath=students\"\n",
    "\n",
    "# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=1FRea6q6kpc4QP1Cfs83BofK5/TXSPDdh+AEhBFwpEg=;EntityPath=at_loandaytransactions\"\n",
    "\n",
    "# connectionString = \"Endpoint=sb://edp-dev-eventhub-ns01.servicebus.windows.net/;SharedAccessKeyName=ADB_SAS;SharedAccessKey=72vrKR8yLJ+hY7WI8QnVqFeLCeP1IKg5g+AEhDZc1sI=;EntityPath=clientloanproposal\"\n",
    "\n",
    "ehConf = {}\n",
    "ehConf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "ehConf[\"eventhubs.consumerGroup\"] = \"$Default\"\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "streaming_df = (\n",
    "    spark.readStream.format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# streaming_df.printSchema()\n",
    "display(streaming_df)\n",
    "\n",
    "###This will display the incoming \"Binary\" code in \"JSON\"\n",
    "# streaming_df = streaming_df.withColumn(\"body_json\", expr(\"cast(body as string)\"))\n",
    "# display(streaming_df.select(\"body\"))\n",
    "\n",
    "# json_schema = StructType([\n",
    "#     StructField(\"table\", StringType(), True),\n",
    "#     StructField(\"op_type\", StringType(), True),\n",
    "#     StructField(\"op_ts\", StringType(), True),\n",
    "#     StructField(\"current_ts\", StringType(), True),\n",
    "#     StructField(\"pos\", StringType(), True),\n",
    "#     StructField(\"after\", StructType([\n",
    "#         StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "#         StructField(\"FIRST_NAME\", StringType(), True),\n",
    "#         StructField(\"LAST_NAME\", StringType(), True),\n",
    "#         StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "#         StructField(\"GENDER\", StringType(), True),\n",
    "#         StructField(\"GRADE\", StringType(), True)\n",
    "#     ]), True),\n",
    "#     StructField(\"before\", StructType([\n",
    "#         StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "#         StructField(\"FIRST_NAME\", StringType(), True),\n",
    "#         StructField(\"LAST_NAME\", StringType(), True),\n",
    "#         StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "#         StructField(\"GENDER\", StringType(), True),\n",
    "#         StructField(\"GRADE\", StringType(), True)\n",
    "#     ]), True)\n",
    "# ])\n",
    "\n",
    "# streaming_df = streaming_df.withColumn(\"body\", F.from_json(streaming_df.body.cast(\"string\"),json_schema))\n",
    "# # streaming_df = streaming_df.select( F.col(\"body.after.STUDENT_ID\"),F.col(\"body.after.FIRST_NAME\"),F.col(\"body.after.LAST_NAME\"),F.col(\"body.after.AGE\"),F.col(\"body.after.GENDER\"),F.col(\"body.after.GRADE\"))\n",
    "\n",
    "# display(streaming_df)\n",
    "# streaming_df = streaming_df.select(\n",
    "#         F.col(\"body.after.STUDENT_ID\"),\n",
    "#         F.col(\"body.after.FIRST_NAME\"),\n",
    "#         F.col(\"body.after.LAST_NAME\"),\n",
    "#         F.col(\"body.after.AGE\"),\n",
    "#         F.col(\"body.after.GENDER\"),\n",
    "#         F.col(\"body.after.GRADE\")\n",
    "#     ) \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"/FileStore/checkpoint/tweet3\") \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .table(\"delta_table\") \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3297a0-34ad-46a8-80af-213277e792ba",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\nProcess End\nProcessing...\nProcess End\nProcessing...\nProcess End\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Working code to update/insert using python\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from pyspark.sql.functions import get_json_object\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)\n",
    "\n",
    "connectionString = \"Endpoint=sb://eventhub-cloud.servicebus.windows.net/;SharedAccessKeyName=student-policy;SharedAccessKey=6r6Kn56jGQRjLgTZMBRi0YJckPiz9fbAj+AEhHzIcMY=;EntityPath=students\"\n",
    "\n",
    "ehConf = {}\n",
    "ehConf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "ehConf[\"eventhubs.consumerGroup\"] = \"$Default\"\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "streaming_df = (\n",
    "    spark.readStream.format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"table\", StringType(), True),\n",
    "    StructField(\"op_type\", StringType(), True),\n",
    "    StructField(\"op_ts\", StringType(), True),\n",
    "    StructField(\"current_ts\", StringType(), True),\n",
    "    StructField(\"pos\", StringType(), True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"body\", F.from_json(streaming_df.body.cast(\"string\"),json_schema))\n",
    "\n",
    "\n",
    "display(streaming_df)\n",
    "\n",
    "def upsert_delta_table(microBatchDF, batchId) :\n",
    "    print(\"Processing...\")\n",
    "    delta_table = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/delta_table\")\n",
    "    delta_table.alias(\"current\").merge(microBatchDF.alias(\"updates\"), \"current.STUDENT_ID = updates.STUDENT_ID\")\\\n",
    "                .whenMatchedUpdate(\n",
    "                    set= {\n",
    "                        \"FIRST_NAME\" : \"updates.FIRST_NAME\",\n",
    "                        \"LAST_NAME\" : \"updates.LAST_NAME\" ,\n",
    "                        \"AGE\": \"updates.AGE\",\n",
    "                        \"GENDER\": \"updates.GENDER\",\n",
    "                        \"GRADE\": \"updates.GRADE\"\n",
    "                    }\n",
    "                ) \\\n",
    "                .whenNotMatchedInsertAll().execute()\n",
    "    print(\"Process End\")\n",
    "\n",
    "display(streaming_df)\n",
    "\n",
    "query = streaming_df.select(\n",
    "    F.col(\"body.after.STUDENT_ID\").alias(\"STUDENT_ID\"),\n",
    "    F.col(\"body.after.FIRST_NAME\").alias(\"FIRST_NAME\"),\n",
    "    F.col(\"body.after.LAST_NAME\").alias(\"LAST_NAME\"),\n",
    "    F.col(\"body.after.AGE\").alias(\"AGE\"),\n",
    "    F.col(\"body.after.GENDER\").alias(\"GENDER\"),\n",
    "    F.col(\"body.after.GRADE\").alias(\"GRADE\")\n",
    ") \\\n",
    ".writeStream \\\n",
    ".foreachBatch(upsert_delta_table) \\\n",
    ".outputMode(\"update\") \\\n",
    ".option(\"checkpointLocation\", \"/FileStore/checkpoint/tweet3\") \\\n",
    ".trigger(processingTime='0 seconds') \\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f2eabfa5-6c0e-4571-b334-cc3b7a0e9b02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\nI\nin insert/update\nProcess End\nProcessing...\nI\nin insert/update\nProcess End\nProcessing...\nI\nin insert/update\nProcess End\nProcessing...\nU\nin insert/update\nProcess End\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Working code to update/insert/Delete using python \n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from pyspark.sql.functions import get_json_object\n",
    "from delta.tables import DeltaTable\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)\n",
    "\n",
    "connectionString = \"Endpoint=sb://eventhub-cloud.servicebus.windows.net/;SharedAccessKeyName=newStudent;SharedAccessKey=wT5I/O5XMbt7Zpb9SO7qNtlyazfTleLPQ+AEhCrGg7c=;EntityPath=students\"\n",
    "\n",
    "ehConf = {}\n",
    "ehConf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "ehConf[\"eventhubs.consumerGroup\"] = \"$Default\"\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "streaming_df = (\n",
    "    spark.readStream.format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"table\", StringType(), True),\n",
    "    StructField(\"op_type\", StringType(), True),\n",
    "    StructField(\"op_ts\", StringType(), True),\n",
    "    StructField(\"current_ts\", StringType(), True),\n",
    "    StructField(\"pos\", StringType(), True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"body\", F.from_json(streaming_df.body.cast(\"string\"),json_schema))\n",
    "\n",
    "\n",
    "display(streaming_df)\n",
    "\n",
    "def upsert_delta_table(microBatchDF, batchId) :\n",
    "    print(\"Processing...\")\n",
    "    delta_table = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/delta_table\")\n",
    "    operation_type = microBatchDF.select(\"body.op_type\").first()[0]\n",
    "    print(operation_type)\n",
    "    if operation_type == \"D\":\n",
    "       print(\"in delete\")\n",
    "       STUDENT_ID = microBatchDF.select(\"body.before.STUDENT_ID\").first()[0]\n",
    "       delta_table.delete(F.col(\"STUDENT_ID\") == STUDENT_ID)\n",
    "    else :\n",
    "        print(\"in insert/update\")\n",
    "        # microBatchDF = microBatchDF.select(\n",
    "        #     F.col(\"body.after.STUDENT_ID\").alias(\"STUDENT_ID\"),\n",
    "        #     F.col(\"body.after.FIRST_NAME\").alias(\"FIRST_NAME\"),\n",
    "        #     F.col(\"body.after.LAST_NAME\").alias(\"LAST_NAME\"),\n",
    "        #     F.col(\"body.after.AGE\").alias(\"AGE\"),\n",
    "        #     F.col(\"body.after.GENDER\").alias(\"GENDER\"),\n",
    "        #     F.col(\"body.after.GRADE\").alias(\"GRADE\")\n",
    "        # ) \n",
    "        microBatchDF = microBatchDF.select(\"body.after.*\")\n",
    "        delta_table.alias(\"current\").merge(microBatchDF.alias(\"updates\"), \"current.STUDENT_ID = updates.STUDENT_ID\")\\\n",
    "                    .whenMatchedUpdate(\n",
    "                        set= {\n",
    "                            \"FIRST_NAME\" : \"updates.FIRST_NAME\",\n",
    "                            \"LAST_NAME\" : \"updates.LAST_NAME\" ,\n",
    "                            \"AGE\": \"updates.AGE\",\n",
    "                            \"GENDER\": \"updates.GENDER\",\n",
    "                            \"GRADE\": \"updates.GRADE\"\n",
    "                        }\n",
    "                    ) \\\n",
    "                    .whenNotMatchedInsertAll().execute()\n",
    "    print(\"Process End\")\n",
    "\n",
    "\n",
    "\n",
    "query = streaming_df.writeStream \\\n",
    ".foreachBatch(upsert_delta_table) \\\n",
    ".outputMode(\"update\") \\\n",
    ".option(\"checkpointLocation\", \"/FileStore/checkpoint/tweet3\") \\\n",
    ".trigger(processingTime='0 seconds') \\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2824fe2c-1ea7-46a4-8db2-5f72616e4e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- body: binary (nullable = true)\n |-- partition: string (nullable = true)\n |-- offset: string (nullable = true)\n |-- sequenceNumber: long (nullable = true)\n |-- enqueuedTime: timestamp (nullable = true)\n |-- publisher: string (nullable = true)\n |-- partitionKey: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- systemProperties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generic code\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from pyspark.sql.functions import get_json_object\n",
    "from delta.tables import DeltaTable\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)\n",
    "\n",
    "connectionString = \"Endpoint=sb://eventhub-cloud.servicebus.windows.net/;SharedAccessKeyName=newStudent;SharedAccessKey=wT5I/O5XMbt7Zpb9SO7qNtlyazfTleLPQ+AEhCrGg7c=;EntityPath=students\"\n",
    "ehConf = {}\n",
    "ehConf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "# ehConf[\"eventhubs.connectionString\"] = \"Endpoint=sb://eventhub-cloud.servicebus.windows.net/;SharedAccessKeyName=newStudent;SharedAccessKey=wT5I/O5XMbt7Zpb9SO7qNtlyazfTleLPQ+AEhCrGg7c=;EntityPath=students\"\n",
    "ehConf[\"eventhubs.consumerGroup\"] = \"$Default\"\n",
    "\n",
    "# Read streaming data from Event Hubs\n",
    "streaming_df = (\n",
    "    spark.readStream.format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"table\", StringType(), True),\n",
    "    StructField(\"op_type\", StringType(), True),\n",
    "    StructField(\"op_ts\", StringType(), True),\n",
    "    StructField(\"current_ts\", StringType(), True),\n",
    "    StructField(\"pos\", StringType(), True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"STUDENT_ID\", IntegerType(), True),\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True),\n",
    "        StructField(\"AGE\", IntegerType(), True),  # Assuming AGE is an integer\n",
    "        StructField(\"GENDER\", StringType(), True),\n",
    "        StructField(\"GRADE\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "streaming_df.printSchema()\n",
    "# print (streaming_df)\n",
    "\n",
    "streaming_df = streaming_df.withColumn(\"body\", F.from_json(streaming_df.body.cast(\"string\"),json_schema))\n",
    "# # streaming_df = streaming_df.withColumn(\"body\", streaming_df[\"body\"].cast(\"string\"))\n",
    "display(streaming_df)\n",
    "\n",
    "def upsert_delta_table(microBatchDF, batchId) :\n",
    "    print(\"Processing...\")\n",
    "    print(microBatchDF)\n",
    "    delta_table = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/delta_table\")\n",
    "    operation_type = microBatchDF.select(\"body.op_type\").first()[0]\n",
    "    print(operation_type)\n",
    "    if operation_type == \"D\":\n",
    "       print(\"in delete\")\n",
    "       STUDENT_ID = microBatchDF.select(\"body.before.STUDENT_ID\").first()[0]\n",
    "       delta_table.delete(F.col(\"STUDENT_ID\") == STUDENT_ID)\n",
    "    else :\n",
    "        print(\"in insert/update\")\n",
    "\n",
    "        #Get table colume from Dataframe\n",
    "        microBatchDF = microBatchDF.select(\"body.after.*\")\n",
    "        print(microBatchDF)\n",
    "\n",
    "        #To dictionary.\n",
    "        after_df = microBatchDF\n",
    "        after_dict = after_df.first().asDict()\n",
    "        print (after_dict)\n",
    "\n",
    "        # Construct set parameter dynamically using after_dict\n",
    "        set_params = {k: f\"updates.{k}\" for k in after_dict}\n",
    "\n",
    "        delta_table.alias(\"current\").merge(microBatchDF.alias(\"updates\"), \"current.STUDENT_ID = updates.STUDENT_ID\")\\\n",
    "                    .whenMatchedUpdate(\n",
    "                        set=set_params\n",
    "                    ) \\\n",
    "                    .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    print(\"Process End\")\n",
    "\n",
    "\n",
    "query = streaming_df.writeStream \\\n",
    ".foreachBatch(upsert_delta_table) \\\n",
    ".outputMode(\"update\") \\\n",
    ".option(\"checkpointLocation\", \"/FileStore/checkpoint/tweet3\") \\\n",
    ".trigger(processingTime='0 seconds') \\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0120500-e14e-48fd-b708-631a65fb6e46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>STUDENT_ID</th><th>FIRST_NAME</th><th>LAST_NAME</th><th>AGE</th><th>GENDER</th><th>GRADE</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "STUDENT_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "FIRST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LAST_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "AGE",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "GENDER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "GRADE",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from delta_table \n",
    "-- DELETE FROM delta_table;\n",
    "-- DELETE FROM delta.`/user/hive/warehouse/delta_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "98926ccd-df99-47d2-a7d3-f5353f9c6e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This function handles Slowly Changing Dimension (SCD) Type 1 operations.\n",
    "    Args:\n",
    "    source_df (DataFrame): The source incremental data.\n",
    "    table_name (str): The name of the Snapshot table.\n",
    "    Returns:\n",
    "    str: A message indicating whether the operation was successful.\n",
    "\"\"\"\n",
    "    \n",
    "def handle_scd_type_1(source_df, desti_path, object_name, primary_key, updated_by, load_type, object_source, adb_curated_dest_url):\n",
    "    \n",
    "    try:\n",
    "        # Construct the Delta table path\n",
    "        delta_table_path = adb_curated_dest_url + \"/\" + desti_path\n",
    "        \n",
    "        # Check if the Delta table exists\n",
    "        delta_table_exists = DeltaTable.isDeltaTable(spark, delta_table_path)\n",
    "        \n",
    "\n",
    "        # Checking load type\n",
    "        if (load_type in [\"INCREMENTAL\", \"FULL LOAD\"]) and (primary_key is not None or (isinstance(primary_key, str) and primary_key.strip() != '') or (isinstance(primary_key, list) and len(primary_key) != 0)):\n",
    "            \n",
    "            if delta_table_exists:\n",
    "                # If Delta table exists, merge the source data with the existing Delta table\n",
    "                delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "                # columns to avoid while updating records\n",
    "                columns_to_exclude = ['ETL_CREATED_DATE']\n",
    "\n",
    "                # Create a dictionary to map columns to be updated\n",
    "                set_dict = {column: f\"updates.{column}\" for column in source_df.columns if column not in columns_to_exclude }\n",
    "\n",
    "                # Check if primary_key is a single primary key or a list of primary keys\n",
    "                if isinstance(primary_key, str):\n",
    "                    condition = f\"current.{primary_key} = updates.{primary_key}\"\n",
    "                elif isinstance(primary_key, (list, tuple)):\n",
    "                    condition = \" AND \".join([f\"current.{key} = updates.{key}\" for key in primary_key])            \n",
    "\n",
    "                delta_table.alias(\"current\").merge(source_df.alias(\"updates\"), condition) \\\n",
    "                    .whenMatchedUpdate(\n",
    "                        condition= \"current.ETL_HASH_COL != updates.ETL_HASH_COL\",\n",
    "                        set= {\n",
    "                            **set_dict,\n",
    "                            \"ETL_CHANGE_FLAG\" : lit(\"U\"),\n",
    "                            \"ETL_LAST_UPDATED_BY\" : lit(updated_by),\n",
    "                            \"ETL_LAST_UPDATED_DATE\" : lit(datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%dT%H:%M:%SZ')) ,\n",
    "                            \"ETL_HASH_COL\": \"updates.ETL_HASH_COL\"\n",
    "                            \n",
    "                        }\n",
    "                    ) \\\n",
    "                    .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "                logger(logger_level_info, f\"In SCD Type 1, the curated delta table has been successfully upserted: {object_name}.\", execution_log_list, filename)\n",
    "                return \"success\"\n",
    "            \n",
    "        # Checking load type\n",
    "        if load_type == \"FULL LOAD\" and (primary_key is None or (isinstance(primary_key, str) and primary_key.strip() == '') or (isinstance(primary_key, list) and len(primary_key) == 0)):\n",
    "\n",
    "            # For FULL LOAD load type truncate and load \n",
    "            max_retries = 10  # Maximum number of retries\n",
    "            retry_delay = 50  # Delay between retries in seconds\n",
    "\n",
    "            retry_count = 0\n",
    "            success = False\n",
    "\n",
    "            while retry_count < max_retries and not success:\n",
    "                try:\n",
    "                    dbutils.fs.rm(delta_table_path, recurse=True)\n",
    "                    spark.sql(f\"DROP TABLE {adb_curated_catlog_schema}.`{object_source}-{object_name}`\")\n",
    "\n",
    "                    # If Delta table is exist, overwrite Delta table and write the source data\n",
    "                    source_df.write.format(\"delta\").mode(\"append\").option(\"path\", delta_table_path).saveAsTable(f\"{adb_curated_catlog_schema}.`{object_source}-{object_name}`\")\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    retry_count += 1\n",
    "                    logger(logger_level_error, f\"An error occurred. Retrying in {retry_delay} seconds...\", execution_log_list, filename)\n",
    "                    time.sleep(retry_delay)\n",
    "\n",
    "            if not success:\n",
    "                logger(logger_level_error, f\"Failed to execute the command after maximum retries is {max_retries}\", execution_log_list, filename)\n",
    "\n",
    "            logger(logger_level_info, f\"In SCD Type 1, the curated delta table has been successfully overwrite for load type {load_type} and for table {object_name}.\", execution_log_list, filename)\n",
    "            return \"success\"\n",
    "\n",
    "        else :\n",
    "\n",
    "            # If Delta table is exist appending source data\n",
    "            source_df.write.format(\"delta\").mode(\"append\").option(\"path\", delta_table_path).saveAsTable(f\"{adb_curated_catlog_schema}.`{object_source}-{object_name}`\")\n",
    "\n",
    "            logger(logger_level_info, f\"In SCD Type 1, the curated delta table has been successfully appended for load type {load_type} and for table {object_name}.\", execution_log_list, filename)\n",
    "            return \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Call the logger function with a log level and message\n",
    "        logger(logger_level_error, f\"There is an error in the SCD Type 1 operation.\", execution_log_list, filename)\n",
    "        print(*execution_log_list,sep='\\n')\n",
    "        raise Exception(f\"An unexpected error occurred:\", e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Recive-EventHub",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
